{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/venv/545bert/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "from bert import EncodedDataset, Classifier_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topk(model, dataloader, topk=1):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dataloader):\n",
    "            label = data['targets'] - torch.ones(data['targets'].shape, dtype=torch.long)\n",
    "            predicted_label = F.softmax(model(data), dim=1)\n",
    "            top_preds, top_idx = predicted_label.topk(k=topk, dim=1)\n",
    "            for i, k in enumerate(top_idx):\n",
    "                total_acc += int(label[i] in k)\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/venv/545bert/lib/python3.8/site-packages/torch/utils/data/datapipes/utils/common.py:24: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded - train: 114000, val: 6000, test: 100\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 64\n",
    "max_len = 64\n",
    "vsplit = 0.05\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "trainset = to_map_style_dataset(train_iter)\n",
    "n_train = int(len(trainset) * vsplit)\n",
    "n_val = len(trainset) - n_train\n",
    "valset, trainset = random_split(trainset, [n_train, n_val])\n",
    "\n",
    "valset = EncodedDataset(valset, tokenizer, max_len, device)\n",
    "trainset = EncodedDataset(trainset, tokenizer, max_len, device)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_iter = AG_NEWS(split='test')\n",
    "testset = to_map_style_dataset(test_iter)\n",
    "testset = EncodedDataset(testset[:100], tokenizer, max_len, device)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Datasets loaded - train: {len(trainset)}, val: {len(valset)}, test: {len(testset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('saved_bert', map_location=torch.device('cpu'))\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullArgSpec(args=['self', 'input_ids', 'attention_mask', 'token_type_ids', 'position_ids', 'head_mask', 'inputs_embeds', 'encoder_hidden_states', 'encoder_attention_mask', 'past_key_values', 'use_cache', 'output_attentions', 'output_hidden_states', 'return_dict'], varargs=None, varkw=None, defaults=(None, None, None, None, None, None, None, None, None, None, None, None, None), kwonlyargs=[], kwonlydefaults=None, annotations={})\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "argspec = inspect.getfullargspec(model.bert.forward)\n",
    "print(argspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_topk(model, testloader, topk=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tec'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _register_embedding_list_hook(model, embeddings_list):\n",
    "    def forward_hook(module, inputs, output):\n",
    "        embeddings_list.append(output.squeeze(0).clone().cpu().detach().numpy())\n",
    "    embedding_layer = model.bert.embeddings.word_embeddings\n",
    "    handle = embedding_layer.register_forward_hook(forward_hook)\n",
    "    return handle\n",
    "\n",
    "def _register_embedding_gradient_hooks(model, embeddings_gradients):\n",
    "    def hook_layers(module, grad_in, grad_out):\n",
    "        # TODO maybe try to do autograd(grad_out, module.weight) or something and append that\n",
    "        embeddings_gradients.append(grad_out[0])\n",
    "        #d2grad = torch.autograd.grad(grad_out, embedding_layer)\n",
    "        #embeddings_gradients.append(d2grad)\n",
    "    embedding_layer = model.bert.embeddings.word_embeddings\n",
    "    hook = embedding_layer.register_full_backward_hook(hook_layers)\n",
    "    return hook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0183e-02, -6.1549e-02, -2.6497e-02, -4.2061e-02,  1.1672e-03,\n",
       "        -2.8272e-02, -4.4500e-02, -2.2465e-02, -4.6553e-03, -8.2129e-02,\n",
       "        -5.0238e-03, -4.6508e-02, -4.9514e-02,  2.1517e-02, -1.6588e-02,\n",
       "        -3.7279e-02, -7.2888e-02, -4.6671e-02,  1.9787e-03, -5.5847e-02,\n",
       "        -2.8919e-02, -2.2304e-02, -4.4846e-03, -1.5506e-02, -1.0986e-01,\n",
       "        -2.6746e-02,  8.3565e-03, -5.3755e-02,  8.1516e-03, -2.5817e-02,\n",
       "        -2.8301e-02, -2.6342e-03, -1.7270e-02, -1.7444e-02, -5.0403e-02,\n",
       "        -5.4036e-02, -3.3925e-02, -1.9397e-02, -6.2235e-02, -1.9178e-03,\n",
       "        -3.0086e-02, -3.1459e-02, -5.0693e-02, -1.8174e-02,  6.8573e-03,\n",
       "        -8.9839e-03, -1.1808e-02, -3.2866e-02, -3.8003e-03, -2.7472e-02,\n",
       "        -3.3144e-02, -1.6076e-02, -5.8682e-02,  1.0107e-01, -2.9100e-02,\n",
       "        -2.4062e-02, -1.5432e-02,  5.2106e-03, -2.3103e-03,  4.4728e-03,\n",
       "        -1.1664e-02, -1.4309e-02,  1.0915e-01, -4.0001e-02, -2.9073e-02,\n",
       "        -1.1655e-02, -2.0877e-02, -3.0113e-02, -6.7081e-03, -3.3477e-02,\n",
       "        -3.2437e-02, -2.2434e-02, -4.2970e-02, -6.0056e-02, -7.4236e-02,\n",
       "        -1.3895e-02, -6.1835e-02,  1.0269e-01, -3.9120e-03, -4.6541e-02,\n",
       "        -3.0964e-02, -3.0767e-02,  2.5584e-03, -1.2184e-02, -1.6253e-02,\n",
       "        -8.0201e-03, -1.9821e-02, -5.0253e-02, -1.4750e-02, -1.5226e-02,\n",
       "        -5.7067e-02,  1.5486e-02, -3.9450e-02, -1.6222e-02, -1.8338e-02,\n",
       "        -5.4316e-02,  1.0489e-01, -2.2203e-02, -2.7777e-02, -4.5925e-02,\n",
       "        -1.5566e-02, -1.9080e-02,  1.0311e-02, -3.6669e-02, -4.7115e-02,\n",
       "         1.0181e-01, -4.6930e-02, -1.2128e-02,  6.5257e-03, -1.2827e-02,\n",
       "        -3.8169e-02, -6.9440e-03, -1.1407e-02, -3.5235e-02, -1.9060e-02,\n",
       "        -1.0307e-02, -1.5898e-02, -4.1200e-02, -4.6643e-02, -1.6302e-02,\n",
       "        -6.0176e-03,  1.3129e-01, -2.1036e-02, -2.2117e-02, -1.4402e-01,\n",
       "        -2.8599e-02, -1.3696e-02, -1.6189e-01, -2.8067e-02, -4.7525e-02,\n",
       "         1.5784e-03, -2.8012e-02,  1.2878e-01,  1.0251e-02, -4.4899e-02,\n",
       "        -1.5743e-02, -1.8824e-02, -2.2676e-02, -1.0056e-02,  1.0403e-01,\n",
       "        -2.7887e-02, -3.1321e-02, -4.4232e-02, -2.5753e-03, -2.5393e-02,\n",
       "        -1.6916e-02, -1.3631e-02, -7.0404e-02, -5.0128e-02, -4.1968e-02,\n",
       "        -1.9662e-02, -2.9982e-02, -9.7740e-02, -2.7181e-02, -4.2037e-02,\n",
       "        -4.5557e-02, -1.0865e-02, -2.0161e-02, -2.8992e-02,  1.0530e-01,\n",
       "        -2.8693e-02, -3.2648e-02, -2.6289e-02, -4.8735e-02, -3.3725e-02,\n",
       "        -3.6585e-02, -2.3407e-03, -1.2749e-02,  1.2790e-01, -3.8057e-02,\n",
       "        -4.1076e-02,  8.9528e-03,  3.8439e-03, -7.4162e-03, -4.3910e-03,\n",
       "         1.1115e-01, -2.6632e-02, -3.8905e-02, -2.6805e-03, -2.5154e-03,\n",
       "        -1.4362e-02, -3.3516e-02, -7.3486e-02, -2.2472e-02, -2.1929e-02,\n",
       "        -4.9053e-02, -1.3212e-02, -2.8033e-02, -2.1171e-02, -2.5870e-02,\n",
       "        -2.0398e-02, -3.1615e-02, -7.5224e-02, -3.3260e-02, -4.6676e-02,\n",
       "        -4.7785e-02, -4.6732e-02, -3.5637e-02, -5.9978e-02, -5.0052e-02,\n",
       "        -6.1737e-02, -7.5817e-03,  9.6870e-02, -6.1890e-02, -2.1407e-02,\n",
       "        -1.2563e-02, -3.6797e-02, -3.3287e-02, -3.4421e-02, -1.3894e-02,\n",
       "        -8.8133e-03, -7.1193e-03, -2.9318e-02, -2.4419e-02, -5.7806e-03,\n",
       "         1.0094e-01,  3.6177e-02, -9.4556e-03,  1.0957e-01, -1.9263e-02,\n",
       "        -1.8295e-02, -1.3713e-02, -3.5285e-02, -2.1754e-02, -6.0722e-02,\n",
       "         7.4449e-02, -3.6236e-02, -3.5528e-02, -2.9701e-02, -4.3306e-02,\n",
       "        -2.8259e-02, -3.1988e-02, -7.5381e-02, -3.7412e-02, -2.5376e-02,\n",
       "        -1.5048e-02, -1.2630e-02,  3.0254e-03, -1.8867e-02, -3.7209e-02,\n",
       "        -4.2477e-02, -2.8853e-02, -4.1193e-02, -5.3924e-02, -4.0215e-02,\n",
       "         1.0739e-01, -8.1261e-03, -6.7805e-02, -1.0442e-02, -5.9944e-03,\n",
       "        -5.0754e-02, -3.4951e-02, -1.9209e-02,  2.4028e-02, -2.3215e-02,\n",
       "         9.1592e-03, -1.5822e-02, -3.6030e-02, -1.2390e-03, -1.5653e-02,\n",
       "        -2.5223e-02, -4.3194e-02,  3.0492e-04, -2.2742e-02, -6.7213e-02,\n",
       "        -1.3190e-02, -9.8063e-03, -3.9939e-02, -3.4624e-02, -3.2686e-02,\n",
       "        -6.0179e-02, -1.7002e-02,  6.2076e-03, -3.9388e-02, -3.4796e-02,\n",
       "        -2.0594e-02, -1.0612e-02, -4.9301e-02, -1.1104e-02, -5.2989e-02,\n",
       "        -2.8889e-02, -3.0242e-02, -2.2861e-02,  8.7022e-02, -5.0955e-03,\n",
       "        -4.8337e-02, -3.7352e-02, -3.6109e-02, -3.7013e-02, -2.8155e-02,\n",
       "        -7.7945e-02,  1.0369e-01, -3.3813e-03, -3.2303e-02, -5.1843e-02,\n",
       "        -1.2638e-02,  7.5395e-02, -4.1105e-02, -8.1194e-02, -3.7080e-02,\n",
       "        -2.9148e-02, -4.2149e-02, -2.0645e-02, -1.5752e-02, -6.2624e-02,\n",
       "        -9.1115e-03, -9.3606e-03, -3.9015e-02,  7.3948e-02, -6.6416e-02,\n",
       "        -2.1531e-02,  1.0344e-01, -1.9789e-02, -5.1098e-02, -4.5898e-02,\n",
       "        -5.9087e-02, -5.4325e-02, -2.0398e-02, -9.4863e-03,  1.0731e-01,\n",
       "        -3.6582e-02, -1.0492e-02, -2.4951e-02, -4.3366e-02, -5.0797e-02,\n",
       "        -7.4541e-03, -1.5912e-02, -3.7816e-02, -8.8382e-03,  9.4313e-03,\n",
       "        -2.6761e-02,  9.2137e-02, -6.0993e-02, -2.4813e-02, -1.7906e-02,\n",
       "        -1.6638e-02,  3.4132e-03, -4.7958e-02, -1.1274e-02, -1.4764e-02,\n",
       "        -2.9976e-02, -3.1712e-02, -3.1065e-02, -2.7403e-02,  3.8099e-03,\n",
       "         1.1391e-01, -3.0056e-02, -7.2062e-02, -4.2530e-02, -6.7686e-03,\n",
       "        -8.2922e-02, -2.2100e-02, -1.5664e-02,  8.6857e-02, -4.2564e-02,\n",
       "        -3.6770e-02, -3.2660e-02, -3.1075e-02, -1.7221e-02, -1.8290e-02,\n",
       "        -9.2484e-03, -4.2167e-02, -8.0546e-03, -3.2582e-02, -1.3629e-02,\n",
       "        -3.9236e-02, -3.0365e-02, -3.1991e-02, -4.8070e-02, -4.5202e-02,\n",
       "        -5.9905e-03, -6.9557e-02,  5.1678e-03, -3.6334e-02, -6.4922e-02,\n",
       "        -8.3255e-02, -2.8914e-02, -3.3091e-02, -2.6676e-02, -9.9082e-03,\n",
       "        -1.2852e-02, -4.4699e-02, -3.9713e-02, -1.3178e-02, -2.7917e-02,\n",
       "        -4.2105e-02, -1.1465e-02, -1.8148e-02, -2.5464e-02, -1.8177e-02,\n",
       "        -6.4978e-02, -3.5865e-02, -4.0396e-02, -2.4917e-02, -3.5268e-02,\n",
       "        -2.9050e-02, -2.9837e-02, -2.9454e-02, -5.8591e-02, -3.8190e-02,\n",
       "        -9.9292e-03, -1.4688e-01, -5.0589e-02, -4.6752e-02, -2.6712e-02,\n",
       "        -1.7390e-02, -3.5716e-02, -1.3478e-02, -1.9240e-02, -5.2646e-02,\n",
       "        -4.4020e-02, -4.3754e-02, -3.4104e-02, -6.1761e-02, -2.2163e-02,\n",
       "        -5.9165e-03, -4.4477e-02, -6.0515e-02, -1.6442e-02, -1.3964e-01,\n",
       "        -1.3091e-02,  2.9891e-03, -2.8450e-02, -4.0221e-02, -6.4843e-02,\n",
       "        -2.3305e-02, -2.1295e-02, -5.9680e-02, -2.7242e-02, -8.1213e-02,\n",
       "        -2.3816e-02, -3.1888e-02, -2.2602e-02,  7.1650e-04, -1.4583e-03,\n",
       "        -3.2853e-02, -1.5436e-02, -1.8040e-02, -4.0737e-03, -1.2500e-02,\n",
       "        -2.2526e-02,  6.1890e-03, -3.6514e-02,  1.1811e-01, -4.8195e-02,\n",
       "        -2.3736e-02, -2.8025e-02, -5.5344e-03, -5.0422e-02, -5.0358e-02,\n",
       "        -2.3664e-02, -2.4511e-02, -2.4658e-02, -1.6584e-02, -1.1389e-03,\n",
       "        -3.4519e-02, -4.2772e-02, -3.7253e-02, -2.1710e-02, -3.2764e-02,\n",
       "        -3.3882e-02,  8.0421e-02, -4.3115e-02, -3.1928e-03, -2.6939e-02,\n",
       "        -3.9356e-02, -3.5763e-02, -1.8438e-02,  4.0275e-03, -4.3807e-02,\n",
       "        -2.5287e-02, -7.5479e-02, -5.1585e-02, -5.5813e-03, -2.8903e-02,\n",
       "        -1.0321e-04, -1.7794e-02, -3.1067e-02, -2.9924e-02, -2.5176e-03,\n",
       "        -2.8445e-02, -9.2819e-03,  1.0094e-01,  1.6806e-03, -1.2409e-02,\n",
       "        -2.0125e-03,  2.8797e-04, -2.6442e-02, -1.9636e-02, -3.3840e-02,\n",
       "        -2.9655e-02, -3.7720e-02, -2.6476e-02, -3.5638e-02, -1.9039e-02,\n",
       "        -1.4205e-02,  3.4499e-02, -2.1957e-03, -5.5352e-02, -6.9782e-02,\n",
       "         5.3646e-03, -4.0639e-02, -3.7305e-02, -4.5545e-02, -8.0159e-02,\n",
       "        -5.8593e-02, -4.1835e-02, -7.8558e-03, -1.6702e-02, -4.3946e-02,\n",
       "        -1.9233e-02, -6.2756e-02, -3.7755e-02, -4.1372e-02, -2.3529e-02,\n",
       "        -3.5586e-02, -1.5953e-02,  1.0166e-01, -1.4708e-02, -3.0660e-02,\n",
       "        -4.6192e-02, -4.9449e-03, -1.2994e-03, -8.7631e-03, -4.4982e-03,\n",
       "        -2.7307e-02, -2.8870e-02, -3.4730e-02, -3.5924e-02,  1.3086e-01,\n",
       "        -5.1444e-02, -6.7933e-02, -2.5930e-02, -1.8110e-02, -1.4634e-02,\n",
       "        -1.8420e-02, -2.7314e-02, -3.9060e-02, -2.7538e-02, -1.3160e-02,\n",
       "        -3.6749e-02,  9.3712e-02, -1.2460e-02, -3.8781e-02, -3.6931e-03,\n",
       "         1.1223e-01,  1.5086e-03, -2.8322e-02, -5.1344e-02, -2.8439e-02,\n",
       "        -3.5813e-02, -1.5584e-02, -2.9601e-02, -1.4666e-02, -1.9192e-02,\n",
       "        -3.4605e-02, -9.5308e-03, -8.8686e-03, -3.9085e-02, -2.5009e-02,\n",
       "        -2.9764e-02, -1.6883e-02, -1.3655e-03, -3.1188e-02, -1.9257e-02,\n",
       "        -2.5113e-02, -5.1900e-02, -4.0914e-02, -7.4950e-02, -1.0425e-02,\n",
       "        -2.3747e-02, -5.3528e-02, -1.9618e-02, -1.5790e-02,  3.6146e-04,\n",
       "        -7.4964e-03, -3.5127e-02,  1.1713e-01, -1.6412e-02, -4.4679e-02,\n",
       "        -1.3890e-02, -6.1886e-02, -3.5083e-02,  1.5211e-03, -8.0316e-03,\n",
       "         3.2548e-03, -3.5813e-02, -2.9009e-02, -4.5562e-02, -8.6372e-02,\n",
       "        -3.0847e-02, -6.5691e-02, -3.0470e-02, -4.8908e-02, -1.2185e-02,\n",
       "         6.3106e-03, -3.1242e-02, -1.4366e-02, -5.4990e-02, -2.6072e-02,\n",
       "        -2.3195e-02, -1.9134e-02, -6.5817e-02,  1.3134e-02, -8.6765e-03,\n",
       "        -2.8602e-02, -6.6840e-03, -6.0905e-02, -4.5329e-02, -1.7936e-02,\n",
       "        -4.6084e-02, -6.0692e-02, -1.4765e-02, -1.0674e-02,  1.0575e-01,\n",
       "        -4.6400e-02, -1.3616e-02, -1.9578e-02, -3.8178e-02, -1.6190e-03,\n",
       "        -2.2568e-02, -3.8275e-02, -2.2664e-02, -3.9251e-02, -2.4218e-02,\n",
       "        -3.9837e-02, -3.9683e-02, -3.8449e-02, -8.9372e-03, -3.0485e-02,\n",
       "        -5.8538e-02, -9.6671e-03, -1.1454e-02, -3.5158e-02, -7.5105e-03,\n",
       "        -2.0650e-02, -4.6145e-03, -4.1925e-02, -1.5284e-02, -4.3931e-02,\n",
       "        -6.5469e-03, -5.1608e-02, -4.4916e-02,  3.0051e-03, -8.5406e-03,\n",
       "        -1.0299e-02, -2.7708e-02, -2.9062e-02, -1.2142e-02,  5.7758e-03,\n",
       "        -6.1638e-02, -2.3828e-02, -2.0867e-02, -5.1357e-02,  1.3327e-01,\n",
       "        -4.2220e-03, -1.4068e-02, -2.8133e-02, -1.0857e-02, -4.2404e-02,\n",
       "        -4.2291e-02, -5.4132e-02, -2.7479e-02, -4.1749e-02, -4.4311e-02,\n",
       "        -3.3302e-02, -3.9388e-03, -5.2169e-02, -2.4235e-02, -1.6820e-02,\n",
       "         1.2338e-01, -5.1071e-02, -4.6809e-02, -1.5621e-02, -2.8104e-02,\n",
       "        -2.3201e-02, -1.9918e-02, -3.0640e-02, -3.3774e-02, -9.1381e-03,\n",
       "        -1.9826e-03, -3.7010e-02, -3.4161e-02, -2.5863e-02, -4.4537e-02,\n",
       "        -2.1705e-02, -1.4065e-02, -2.7418e-02, -4.1339e-02, -9.9585e-03,\n",
       "        -3.6958e-02,  5.8991e-03, -3.1152e-02, -1.4280e-02, -3.5814e-02,\n",
       "        -1.4860e-02,  3.9596e-03, -1.1289e-02, -4.8315e-02,  5.8197e-02,\n",
       "        -8.1996e-03,  1.4357e-03, -3.0713e-02, -6.0927e-02, -5.6728e-02,\n",
       "        -3.5902e-02, -4.0188e-03, -2.9491e-03, -3.7171e-02, -4.8878e-02,\n",
       "        -6.3957e-03, -3.7394e-02, -2.1048e-02, -3.4480e-02, -1.0230e-02,\n",
       "        -2.0924e-02, -2.5572e-02,  2.5012e-03, -5.4340e-02, -2.3479e-03,\n",
       "        -1.6723e-02, -2.4130e-02, -5.0779e-02, -2.7017e-02, -3.9475e-02,\n",
       "        -4.3564e-02, -3.7715e-03, -3.5978e-02, -6.0178e-02, -2.2083e-02,\n",
       "        -3.9697e-02, -9.6672e-03, -1.9678e-02, -4.4540e-02, -6.2129e-02,\n",
       "        -1.3934e-03, -1.5755e-04, -7.7517e-03, -7.1707e-02,  8.3854e-04,\n",
       "        -4.0077e-02, -3.6744e-02, -1.5602e-02, -6.2337e-02, -1.9976e-02,\n",
       "        -2.9310e-02, -3.3579e-03, -3.0580e-02, -6.0318e-03, -1.6074e-02,\n",
       "        -2.1336e-02, -5.3142e-02, -6.7147e-03, -4.2129e-02, -4.7328e-02,\n",
       "         7.8782e-03, -6.8284e-02, -3.2570e-02, -4.3651e-02, -7.7066e-03,\n",
       "        -9.2494e-03, -3.1207e-02, -3.6753e-02, -3.8042e-02, -1.4242e-02,\n",
       "        -1.9854e-02, -3.7210e-02, -9.7515e-03], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.bert.embeddings.word_embeddings.parameters())[0][0]\n",
    "model.bert.embeddings.word_embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bert_saliency_map(X, label, model, loss):\n",
    "    torch.enable_grad()\n",
    "\n",
    "    embeddings_list = []\n",
    "    handle = _register_embedding_list_hook(model, embeddings_list)\n",
    "    embeddings_gradients = []\n",
    "    hook = _register_embedding_gradient_hooks(model, embeddings_gradients)\n",
    "\n",
    "    model.zero_grad()\n",
    "    A = model(X)\n",
    "    #pred_label_ids = np.argmax(A.logits[0].detach().numpy())\n",
    "    A[0,label].backward()\n",
    "    handle.remove()\n",
    "    hook.remove()\n",
    "\n",
    "\n",
    "    saliency_grad = embeddings_gradients[0].detach().cpu().numpy()  \n",
    "    saliency_grad = np.sum(saliency_grad[0] * embeddings_list[0], axis=1)\n",
    "    norm = np.linalg.norm(saliency_grad, ord=1)\n",
    "    # saliency_grad = [e / norm for e in saliency_grad] \n",
    "    saliency = np.abs(np.array([e / norm for e in saliency_grad]))\n",
    "    return saliency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bert_saliency_map_d2(X, label, model, loss):\n",
    "    torch.enable_grad()\n",
    "\n",
    "    embeddings_list = []\n",
    "    handle = _register_embedding_list_hook(model, embeddings_list)\n",
    "    embeddings_gradients = []\n",
    "    hook = _register_embedding_gradient_hooks(model, embeddings_gradients)\n",
    "\n",
    "    model.zero_grad()\n",
    "    A = model(X)\n",
    "    #pred_label_ids = np.argmax(A.logits[0].detach().numpy())\n",
    "    #print(A[0,label], X)\n",
    "    #first_derivative = torch.autograd.grad(A[0,label], X['ids'], create_graph=True)\n",
    "    #second_derivative = torch.autograd.grad(first_derivative, X)[0]\n",
    "\n",
    "    A[0,label].backward()\n",
    "    handle.remove()\n",
    "    hook.remove()\n",
    "\n",
    "    #print(embeddings_list[0].requires_grad)\n",
    "    print(embeddings_gradients[1])\n",
    "\n",
    "\n",
    "    #grad2 = torch.autograd.grad(embeddings_gradients[0], embeddings_list[0], create_graph=True)\n",
    "\n",
    "    #print(embeddings_gradients)\n",
    "\n",
    "    #saliency_grad = embeddings_gradients[0].detach().cpu().numpy()  \n",
    "    saliency_grad = embeddings_gradients[1].detach().cpu().numpy()  \n",
    "    print(f'd2 saliency grad shape: {saliency_grad.shape}')\n",
    "    saliency_grad = np.sum(saliency_grad[0] * embeddings_list[0], axis=1)\n",
    "    norm = np.linalg.norm(saliency_grad, ord=1)\n",
    "    # saliency_grad = [e / norm for e in saliency_grad] \n",
    "    saliency = np.abs(np.array([e / norm for e in saliency_grad]))\n",
    "    return saliency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_text_saliency_maps(X, y, tokenizer, correct_label, label_dict, model):\n",
    "\n",
    "    prediction = model(X)\n",
    "    predicted_class = prediction.argmax()\n",
    "    print(f'The predicted class is: {label_dict[predicted_class.item()]}, the correct class is: {correct_label}')\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "    saliencies = np.zeros(shape = (X['ids'].shape[1], *y.size()))\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for i, label in enumerate(y):\n",
    "        #saliencies[:, i] = compute_bert_saliency_map(X, label, model, loss)\n",
    "        #saliencies[:, i] = compute_alt_saliency_map(X, label, model, loss)\n",
    "        saliencies[:, i] = compute_bert_saliency_map_d2(X, label, model, loss)\n",
    "        \n",
    "    detoked = np.array(tokenizer.decode(X['ids'].flatten()).split())\n",
    "\n",
    "    N = y.size()[0]\n",
    "    pad_idx = np.min(np.where(detoked == '[SEP]')[0])\n",
    "    x_ticks = np.arange(pad_idx+1)\n",
    "    fig, axes = plt.subplots(N, 1, sharex=False, sharey=False, figsize=(8,60))\n",
    "    for i, label in enumerate(y):\n",
    "        axes[i].plot(saliencies[:pad_idx+1, i], x_ticks, '-o')\n",
    "        # axes[i].set_xticklabels(np.linspace(start=0, stop=saliencies[:, i].max(), num=10))\n",
    "        axes[i].set_yticks(ticks=x_ticks, labels=detoked[:pad_idx+1])\n",
    "        axes[i].set_title(f'class \\'{label_dict[label.item()]}\\' prediction')\n",
    "        axes[i].grid()\n",
    "        axes[i].invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.random.randint(low=0, high=len(testset))\n",
    "test_point = testset[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: Business, the correct class is: Sci/Tec\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Embedding' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/Josh Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000010vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m test_point:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000010vscode-remote?line=3'>4</a>\u001b[0m     test_point[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mreshape(test_point[key], shape\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000010vscode-remote?line=5'>6</a>\u001b[0m show_text_saliency_maps(test_point, torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m]), tokenizer, label, idx2label, model)\n",
      "\u001b[1;32m/mnt/c/Users/Josh Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb Cell 12'\u001b[0m in \u001b[0;36mshow_text_saliency_maps\u001b[0;34m(X, y, tokenizer, correct_label, label_dict, model)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000008vscode-remote?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000008vscode-remote?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(y):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000008vscode-remote?line=11'>12</a>\u001b[0m     \u001b[39m#saliencies[:, i] = compute_bert_saliency_map(X, label, model, loss)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000008vscode-remote?line=12'>13</a>\u001b[0m     \u001b[39m#saliencies[:, i] = compute_alt_saliency_map(X, label, model, loss)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000008vscode-remote?line=13'>14</a>\u001b[0m     saliencies[:, i] \u001b[39m=\u001b[39m compute_bert_saliency_map_d2(X, label, model, loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000008vscode-remote?line=15'>16</a>\u001b[0m detoked \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(tokenizer\u001b[39m.\u001b[39mdecode(X[\u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mflatten())\u001b[39m.\u001b[39msplit())\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000008vscode-remote?line=17'>18</a>\u001b[0m N \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m/mnt/c/Users/Josh Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb Cell 11'\u001b[0m in \u001b[0;36mcompute_bert_saliency_map_d2\u001b[0;34m(X, label, model, loss)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000021vscode-remote?line=9'>10</a>\u001b[0m A \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000021vscode-remote?line=10'>11</a>\u001b[0m \u001b[39m#pred_label_ids = np.argmax(A.logits[0].detach().numpy())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000021vscode-remote?line=11'>12</a>\u001b[0m \u001b[39m#print(A[0,label], X)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000021vscode-remote?line=12'>13</a>\u001b[0m \u001b[39m#first_derivative = torch.autograd.grad(A[0,label], X['ids'], create_graph=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000021vscode-remote?line=13'>14</a>\u001b[0m \u001b[39m#second_derivative = torch.autograd.grad(first_derivative, X)[0]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000021vscode-remote?line=15'>16</a>\u001b[0m A[\u001b[39m0\u001b[39;49m,label]\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000021vscode-remote?line=16'>17</a>\u001b[0m handle\u001b[39m.\u001b[39mremove()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000021vscode-remote?line=17'>18</a>\u001b[0m hook\u001b[39m.\u001b[39mremove()\n",
      "File \u001b[0;32m/usr/venv/545bert/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/usr/venv/545bert/lib/python3.8/site-packages/torch/utils/hooks.py:175\u001b[0m, in \u001b[0;36mBackwardHook.setup_output_hook.<locals>.fn.<locals>.hook\u001b[0;34m(_, grad_output)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/utils/hooks.py?line=172'>173</a>\u001b[0m grad_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pack_with_none([], [], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_inputs)\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/utils/hooks.py?line=173'>174</a>\u001b[0m \u001b[39mfor\u001b[39;00m user_hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_hooks:\n\u001b[0;32m--> <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/utils/hooks.py?line=174'>175</a>\u001b[0m     res \u001b[39m=\u001b[39m user_hook(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule, grad_inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad_outputs)\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/utils/hooks.py?line=175'>176</a>\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(el \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m res)):\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/utils/hooks.py?line=176'>177</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mBackward hook for Modules where no input requires \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/utils/hooks.py?line=177'>178</a>\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mgradient should always return None or None for all gradients.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/mnt/c/Users/Josh Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb Cell 8'\u001b[0m in \u001b[0;36m_register_embedding_gradient_hooks.<locals>.hook_layers\u001b[0;34m(module, grad_in, grad_out)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000006vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhook_layers\u001b[39m(module, grad_in, grad_out):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000006vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39m# TODO maybe try to do autograd(grad_out, module.weight) or something and append that\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000006vscode-remote?line=10'>11</a>\u001b[0m     embeddings_gradients\u001b[39m.\u001b[39mappend(grad_out[\u001b[39m0\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000006vscode-remote?line=11'>12</a>\u001b[0m     d2grad \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(grad_out, embedding_layer)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/Josh%20Silverberg/Desktop/Michigan/eecs545/projectcode/bert_saliency.ipynb#ch0000006vscode-remote?line=12'>13</a>\u001b[0m     embeddings_gradients\u001b[39m.\u001b[39mappend(d2grad)\n",
      "File \u001b[0;32m/usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py:239\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=186'>187</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Computes and returns the sum of gradients of outputs with respect to\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=187'>188</a>\u001b[0m \u001b[39mthe inputs.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=188'>189</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=235'>236</a>\u001b[0m \u001b[39m        for your use case. Defaults to ``False``.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=236'>237</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=237'>238</a>\u001b[0m outputs \u001b[39m=\u001b[39m (outputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m(outputs)\n\u001b[0;32m--> <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=238'>239</a>\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39;49m(inputs)\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=239'>240</a>\u001b[0m overridable_args \u001b[39m=\u001b[39m outputs \u001b[39m+\u001b[39m inputs\n\u001b[1;32m    <a href='file:///usr/venv/545bert/lib/python3.8/site-packages/torch/autograd/__init__.py?line=240'>241</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(overridable_args):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Embedding' object is not iterable"
     ]
    }
   ],
   "source": [
    "idx = test_point['targets'].item()-1\n",
    "label = idx2label[idx]\n",
    "for key in test_point:\n",
    "    test_point[key] = torch.reshape(test_point[key], shape=(1,-1))\n",
    "\n",
    "show_text_saliency_maps(test_point, torch.tensor([0, 1, 2, 3]), tokenizer, label, idx2label, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "563462dbe791d5e19c528565928096f49853767f8fe1f86d2fbf01368022040a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('545bert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
